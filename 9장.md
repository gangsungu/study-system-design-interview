## 웹 크롤러 설계
웹 크롤러는 검색엔진에서 널리 쓰는 기술로 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적

콘텐츠는 웹 페이지일수도 있고 이미지나 비디오, PDF 파일일수도 있다

### 읽기 전 간단하게 설계해보기
+ 웹 페이지 하나를 수집한다
+ 메인 URL 아래의 서브 URL을 탐색해야 한다
    - `탐색은 어떻게 진행하는가?`
+ 탐색한 URL을 저장한다
    - 저장한 URL과 중복 체크 진행
        + `중복은 어떻게 체크하는가?`
+ 페이지와 URL을 저장하고 좀 더 빠른 중복체크를 위해 메모리에 캐싱
    - write-through 방식 사용가능

### 놓친 부분
+ 콘텐츠 파서
    - 이상한 웹 페이지는 저장공간을 낭비하고 문제를 일으킬 수 있다
    - 웹 페이지를 그대로 저장하면 저장공간의 낭비가 발생
+ 중복 콘텐츠 체크
    - 웹 페이지를 그대로 비교하는 것이 아닌 해시값을 비교하여 체크
+ URL 필터 추가
    - 접근 제어 목록

### 웹 크롤러 작업 흐름
+ 시작 URL을 미수집 URL 저장소에 저장
+ HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다
+ HTML 다운로더는 도메인 이름 변화기로 URL의 IP 주소를 확인하고 해당 IP 주소로 이동하여 웹 페이지 다운
+ 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 검증
+ 콘텐츠 파싱과 검증이 끝나면 중복 콘텐츠 여부 체크
    - 해당 페이지가 저장소에 있는지 확인
    - 저장소에 없으면 DB에 저장한 뒤, URL 추출기로 전달
+ URL 추출기는 해당 HTML 페이지에서 링크를 선별
+ 선별한 링크를 URL 필터로 전달
+ 필터링 후, 남은 URL로 URL 중복 체크 진행
+ 저장소에 없는 URL은 URL 저장소에 저장 후, 미수집 URL 저장소에 전달

### 탐색은 어떻게 진행하는가?
+ 웹은 유향 그래프(directed graph)
    - 페이지는 노드, 하이퍼링크(URL)은 에지
    - 크롤링 프로세스는 에지를 따라 탐색하는 과정
+ 그래프 탐색
    - 깊이 우선 탐색법(DFS)
    - 너비 우선 탐색법(BFS)

여기서 깊이 우선 탐색법보다 너비 우선 탐색법이 더 좋은 선택지가 될 수 있는데, 그래프 크기가 크면 깊이 우선 탐색법은 깊이(depth)가 얼마나 깊숙하게 들어갈지 가늠하기 어렵다

그래서 크롤러는 BFS를 사용하고 BFS는 FIFO 큐를 사용하는 알고리즘이다
여기엔 두가지 문제점이 있다.

한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아간다
아래의 예를 보면 wikipedia.com 페이지에서 추출한 모든 링크는 내부 링크이며 크롤러는 같은 호스트에 속한 많은 링크를 다운받는다

이 링크들을 병렬로 처리하면 위키피디아 서버는 수많은 요청으로 과부하에 걸리게 된다
이것을 `예의없는 크롤러`라고 한다

```
# https://www.wikipedia.org/robots.txt
User-agent: *
Crawl-delay: 1  # :point_left: 요청 간격 1초 이상 유지하라!
```

실제로 많은 웹사이트는 robots.txt에 크롤링 규칙을 명시한다

```
wikipedia.com > wikipedia.com/page1 > wikipedia.com/page1/1 / wikipedia.com/page1/2
```

BFS 알고리즘은 FIFO 큐 알고리즘이므로 URL 간에 우선순위를 두지 않는다
하지만 모든 웹 페이지가 동등한 중요성을 가지지 않는다
따라서 페이지 순위, 사용자 트래픽의 양, 업데이트 빈도 등을 확인하여 처리순서를 조정하는 것이 필요하다

### 미수집 URL 저장소
동일 웹 사이트에 대해서는 한번에 한 페이지만 요청한다
이 요구사항을 만족시키기 위해서는 웹 사이트의 호스트명과 다운로드를 진행하는 작업 스레드를 몪으면 된다
즉, 각 다운로드 스레드는 별도의 FIFO 큐를 가지고 있다

+ 큐 라우터
    - 같은 호스트에 속한 URL은 언제나 같은 큐로 이동시킨다
+ 매핑 테이블
    - 호스트 이름과 큐 사이의 관계를 보관하는 테이블

    |호스트|큐|
    |--|--|
    |wikipedia.com|a1|
    |apple.com|b1|
+ FIFO 큐
    - 같은 호스트에 속한 URL은 항상 같은 큐에 저장됨
+ 큐 선택기
    - 큐를 순회하면서 URL을 꺼내 작업 스레드에 전달하는 역할
+ 작업 스레드
    - 전달된 URL을 다운로드하는 작업을 수행, 전달된 URL은 순차적으로 처리되며 중간에 지연시간을 넣어 예의없는 크롤러를 방지할 수 있음

### 신선도
웹 페이지는 끊임없이 추가되고 수정, 삭제된다
따라서 다운로드한 페이지여도 주기적으로 갱신할 필요가 있다
변경이력을 사용하거나 우선순위를 활용하여 순위가 높은 페이지는 좀 더 자주 재수집

### HTML 다운로더의 성능 최적화
+ 분산 크롤링
    - 크롤링 작업을 여러 서버에 분산하여 성능을 높이는 방식
+ 도메인 이름 변환 결과 캐시
    - 도메인 이름 변환기는 크롤러 성능의 병목 중 하나
        + DNS 요청을 보내고 결과를 받는 동기적 특성때문
+ 지역성
    - 크롤링 서버를 각 지역에 분산하여 페이지 다운로드를 좀 더 가까운 위치에서 실행시키는 작전
+ 짧은 타임아웃
    - 서버가 빠르게 응답하지 않으면 응답을 계속 기다리지 않고 바로 다음 서버로 이동

### 안정성
+ 안정 해시
    - 다운로더 서버들의 부하 감소
+ 크롤링 상태 및 수집 데이터 저장
    - 장애가 발생하더라도 쉽게 복구가 가능하고 데이터를 확인할 수 있는
+ 예외 처리, 데이터 검증
    - 시스템 오류를 방지하기 위한 방법

### 확장성
모든 시스템은 확장성을 신경써야 한다

+ 중복 콘텐츠
    - 해시나 체크섬을 사용하여 중복 콘텐츠를 탐지
+ 거미 덫
    - spidertrapexample.com/foo/bar/foo/bar/foo/...
        + 이런 종류의 URL은 URL의 최대 길이 제한을 두면 회피가 가능하다
        + 다만 정상적인 URL도 막히는 문제가 발생
+ 데이터 노이즈
    - 광고나 스크립트 코드, 스팸 등을 가치가 없다